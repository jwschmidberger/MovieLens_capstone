##### The movielens project
##### Create EdX set, validation set. This first part is taken from HarvardX EdX website. 

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cral.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]



# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)


rm(dl, ratings, movies, test_index, temp, movielens, removed)


## ----Creating edx_work and edx_cv------------------------------------------------------------------------
test_index2 <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
edx_work <- edx[-test_index2,]
temp <- edx[test_index2,]

edx_cv <- temp %>% 
  semi_join(edx_work, by = "movieId") %>%
  semi_join(edx_work, by = "userId")

# To ensure all movies and users present in edx_cv are also present in edx_work.
removed <- anti_join(temp, edx_cv)
edx_work <- rbind(edx_work, removed)
rm(test_index2, temp)


# Which movie has the highest number of ratings?
edx %>% group_by(title) %>% summarise(count = n()) %>% arrange(desc(count)) %>%
  head(., 10) %>% knitr::kable()


# What is the ranking order of movie ratings. 
edx %>% group_by(rating) %>% summarise(count = n()) %>% arrange(desc(count)) %>%
  knitr::kable()


# Lineplot of the frequency of each rating. 
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) + 
  ggtitle("Line plot of total ratings counts") +
  geom_line()


# Here I define the RMSE function that I will use in some of my assessments of predictive models below.
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

########################################################

##### Now picking up on Section 6 of the Machine Learning module of the course, I will start to process
# the edx and validation datasets generated by the instructions given for this project. 

# The following libraries will be relevant to my work here. 
library(dslabs)
library(tidyverse)
library(dplyr)
library(caret)

##### Most basic model where I assume all ratings are equal to the over all mean of all ratings in edx.
mu_hat <- as.data.frame(mean(edx_work$rating))
rownames(mu_hat) <- c("mu_hat")
mu_hat %>% knitr::kable()

naive_rmse <- RMSE(edx_cv$rating, mu_hat$`mean(edx_work$rating)`)


rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

### "Just the average yields an RMSE of 1.06. Not bad but it should be easy to improve on this. 

### Trying to factor in movie bias.
mu <- mean(edx_work$rating)
movie_avgs <- edx_work %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu))
head(movie_avgs, 10) %>% knitr::kable()


# Visualising the frequency distribution of movie residuals. 
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"),
                     main = "Frequency distribution of movie residuals")

## ----Algorithm 2-----------------------------------------------------------------------------------------
# Calculating the new predictions given the b_i values for each row in edx_cv dataset and adding it to 
# mu. 
predicted_ratings_2 <- mu + edx_cv %>%
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_2_rmse <- RMSE(predicted_ratings_2, edx_cv$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Movie Effect Model", 
                                     RMSE = model_2_rmse))


## ----Algorithm 3-----------------------------------------------------------------------------------------
### Now I am looking at the influence of the user in context of the movie effect (i.e. cumulative effect). 
user_avgs <- edx_work %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu))
predicted_ratings_3 <- edx_cv %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_3_rmse <- RMSE(predicted_ratings_3, edx_cv$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method = "User and Movie Effect Model",
                                     RMSE = model_3_rmse))

# Below is an effor to apply regularisation across the dataset. Modifying ratings that have only a few ratings
# vs many. 
### Looking at top ten movies, and bottom 10 movies in edx.
movie_titles <- edx_work %>% 
  select(movieId, title) %>%
  distinct()
# Top 10
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>%
  select(title, b_i) %>%
  slice(1:10) %>%
  knitr::kable()


# Bottom 10
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>%
  select(title, b_i) %>%
  slice(1:10) %>%
  knitr::kable()

### Now looking at the top 10 movie residual values in edx plus the number of ratings for each movie. 
# It is clear the many of the movies with largest residuals have very few ratings. 
edx_work %>% count(movieId) %>% 
  left_join(movie_avgs)%>% 
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>%
  select(title, b_i, n) %>%
  slice(1:10) %>%
  knitr::kable()

### Application of the regularisation term.
## ----Lambda of 3
lambda <- 3
mu <- mean(edx_work$rating)
movie_reg_avgs <- edx_work %>%
  group_by(movieId) %>% 
  summarise(b_i = sum(rating - mu)/(n() + lambda), n_i = n())
head(movie_reg_avgs, 10) %>% knitr::kable()

# Visualising the difference in b_i values from the original movie_avgs vs the modified movie_reg_avgs.
data_frame(original = movie_avgs$b_i, 
           regularized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)


### Lets look at top 10 using these regularised movie estimates. Now we are seeing more sensible movies
# start to appear, with understandably lower b_i values. 
edx_work %>% count(movieId) %>% 
  left_join(movie_reg_avgs)%>% 
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>%
  select(title, b_i, n) %>%
  slice(1:10) %>%
  knitr::kable()


## ----Algorithm 4-----------------------------------------------------------------------------------------
# Perform prediction on regularised movie b_i. 
predicted_ratings_4 <- edx_cv %>%
  left_join(movie_reg_avgs, by = 'movieId') %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_4_rmse <- RMSE(predicted_ratings_4, edx_cv$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Regularised movie effect", 
                                     RMSE = model_4_rmse))


### Now I need to apply the same calculation for regularised user effects.
user_reg_avgs <- edx_work %>%
  group_by(userId) %>% 
  summarise(b_u = sum(rating - mu)/(n() + lambda), n_u = n())

# Visualising this.
data_frame(original = user_avgs$b_u, 
           regularized = user_reg_avgs$b_u, 
           n = user_reg_avgs$n_u) %>%
  ggplot(aes(original, regularized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)

# Apply regularised user b to prediction calculation.
## ----Algorithm 5-----------------------------------------------------------------------------------------
predicted_ratings_5 <- edx_cv %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_u) %>%
  .$pred

model_5_rmse <- RMSE(predicted_ratings_5, edx_cv$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Regularised user effect", 
                                     RMSE = model_5_rmse))


## ----Algorithm 6-----------------------------------------------------------------------------------------
# Apply regularised user b to whole prediction calculation, including reg movie effect.
predicted_ratings_6 <- edx_cv %>%
  left_join(movie_reg_avgs, by = 'movieId') %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_6_rmse <- RMSE(predicted_ratings_6, edx_cv$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Reg movie and user effect", 
                                     RMSE = model_6_rmse))


# This yields an RMSE of 0.8844 which is already pretty good. 

# Note that lambda is a parameter. We can use cross validation to tune it. 
lambda <- seq(0, 10, 0.25)


## --------------------------------------------------------------------------------------------------------
# Running the refinement of best lambda value. 
set.seed(1, sample.kind = "Rounding")
rmses <- sapply(lambda, function(l){
  mu <- mean(edx_work$rating)
  b_i <- edx_work %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+l))
  b_u <- edx_work %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings<- edx_cv %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>% 
    .$pred
  return(RMSE(predicted_ratings, edx_cv$rating))
})
plot(lambda, rmses, main = "Tuning Lambda")

## ----echo=F----------------------------------------------------------------------------------------------
print("The optimal Lambda")
lambda[which.min(rmses)]

### The best lambda value is 4.5. 

### Applying this value to the formal prediction. 
## ----Algorithm 7-----------------------------------------------------------------------------------------
b_i <- edx_work %>%
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu)/(n()+4.5))
b_u <- edx_work %>%
  left_join(b_i, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu)/(n()+4.5))
predicted_ratings_7 <- edx_cv %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>% 
  .$pred
model_7_rmse <- RMSE(predicted_ratings_7, edx_cv$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Reg mov & usr with 4.5 l", 
                                     RMSE = model_7_rmse))

# Some tidying up.
rm(edx, predicted_ratings_2, predicted_ratings_3, predicted_ratings_4, predicted_ratings_5, predicted_ratings_6, predicted_ratings_7)


## --------------------------------------------------------------------------------------------------------
### Principal componenent analysis.
# I will apply a pca to a small subset of EDX. 

# Focusing in on the most rated movies (>1000 ratings), and most prolific users (>500 ratings). 
edx_small <- edx_work %>%
  group_by(movieId) %>%
  filter(n() >= 1000) %>% ungroup() %>% 
  group_by(userId) %>%
  filter(n() >= 500) %>% ungroup()


### Now I need to make a matrix out of userId's and movieId's showing corresponding ratings. 
y <- edx_small %>%
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()
y[1:10, 1:5]
rm(edx_small, edx_work)

# Making column 1 the rownames. Then remove column 1. 
rownames(y) <- y[,1]
y <- y[,-1]
tmp <- y
colnames(tmp) <- with(movie_titles, title[match(colnames(tmp), movieId)])
y[1:10, 1:5]


## ----Preparation for PCA---------------------------------------------------------------------------------
### Preparing the matrix for PCA.
y <- sweep(y, 1, rowMeans(y, na.rm = TRUE))
y <- sweep(y, 2, colMeans(y, na.rm = TRUE))

y[is.na(y)] <- 0
y <- sweep(y, 1, rowMeans(y))


## ----PCA-------------------------------------------------------------------------------------------------
pca <- prcomp(y, center = F, scale. = F, retx = T)

# Visualising the PCA. 
plot(pca$sdev, main = "PCA standard deviation")


## --------------------------------------------------------------------------------------------------------
var_explained <- cumsum(pca$sdev^2/sum(pca$sdev^2))
plot(var_explained, main = "PCA variance by PC index")
# Certainly most of the variation is explained by 500 principal components. It is impractical for me to 
# apply all of these using the method I implement below. I will begin by applying a few PC's at a time. 

# Some data visualisation. Looking at the plot of PC1 vs PC2 to get a feel for the sort of relationships
# that exist between movies.
library(ggrepel)
pcs <- data.frame(pca$rotation, name = colnames(tmp))
pcs %>%  ggplot(aes(PC1, PC2)) + geom_point() + 
  geom_text_repel(aes(PC1, PC2, label=name),
                  data = filter(pcs, 
                                PC1 < -0.1 | PC1 > 0.1 | PC2 < -0.075 | PC2 > 0.1))


## ----echo=F----------------------------------------------------------------------------------------------
pcs %>% select(name, PC1) %>% arrange(PC1) %>% slice(1:10) %>%
  knitr::kable()


## ----echo=F----------------------------------------------------------------------------------------------
pcs %>% select(name, PC1) %>% arrange(desc(PC1)) %>% slice(1:10) %>%
  knitr::kable()

rm(tmp)
##### I need to take the x and rotation matrices from pca and expand them to match the dimensions of 
# the edx_cv matrix. That is unique userId (68052) by unique movieId (9728). 
# These expanded matrices will be sparse matrices. 

### Now I need the actual ratings from the edx_cv data set in a 68052 by 9728 matrix.
## ----edx_cv to matrix------------------------------------------------------------------------------------
val_ratings.m <- edx_cv %>%
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()


# Fixing the rownames of edx_cv matrix and removing first column that features userId
rownames(val_ratings.m) <- val_ratings.m[,1]
val_ratings.m <- val_ratings.m[,-1]
colnames(val_ratings.m) <- with(movie_titles, title[match(colnames(val_ratings.m), movieId)])
val_ratings.m[1:10, 5:7]

### I need the lists of all userId's and movieId's in the edx_cv to use as a reference in making my 
# expanded pca sparse matrices.
## --------------------------------------------------------------------------------------------------------
unique_usr_val <- as.matrix(unique(edx_cv$userId)) 
colnames(unique_usr_val) <- c("userId")


## --------------------------------------------------------------------------------------------------------
unique_mov_val <- as.matrix(unique(edx_cv$movieId)) 
colnames(unique_mov_val) <- c("movieId")


# Creating the "User effect" sparse matrix.
pca_x <- pca$x %>% as.data.frame() %>%
  tibble::rownames_to_column(., "userId") %>%
  merge(unique_usr_val, ., by = "userId", all = TRUE)
pca_x <- as.matrix(pca_x[,-1])
pca_x[is.na(pca_x)] <- 0


# Creating the "Principal component" sparse matrix. 
pca_rotation <- pca$rotation %>% as.data.frame() %>%
  tibble::rownames_to_column(., "movieId") %>%
  merge(unique_mov_val, ., by = "movieId", all = TRUE)
pca_rotation <- as.matrix(pca_rotation[,-1])
pca_rotation[is.na(pca_rotation)] <- 0


#### Now to build a large matrix of predictions from where I left off after "Reg mov and usr with 4.5".
# This will have the dimensions that are consistent with unique userId/movidId values from edx_cv.
predictions <- edx_cv %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u)


## ----predictions to matrix-------------------------------------------------------------------------------
y3 <- predictions %>%
  select(userId, movieId, pred) %>%
  spread(movieId, pred) %>%
  as.matrix()
rownames(y3) <- y3[,1]
y3 <- y3[,-1]

# Clean up.
rm(predictions)

# Defining my p and q vectors
# Vectors p are called user effects. Vectors q are called prinipal components. 
# NOTE... both p1 and q1 are converted to matrices, with p1 being 68052 X 1, and q1 1 X 9728 
p1 <- as.matrix(pca_x[,1])
q1 <- matrix(pca_rotation[,1], nrow = 1, byrow = T)
p2 <- as.matrix(pca_x[,2])
q2 <- matrix(pca_rotation[,2], nrow = 1, byrow = T)
p3 <- as.matrix(pca_x[,3])
q3 <- matrix(pca_rotation[,3], nrow = 1, byrow = T)
p4 <- as.matrix(pca_x[,4])
q4 <- matrix(pca_rotation[,4], nrow = 1, byrow = T)
p5 <- as.matrix(pca_x[,5])
q5 <- matrix(pca_rotation[,5], nrow = 1, byrow = T)
p6 <- as.matrix(pca_x[,6])
q6 <- matrix(pca_rotation[,6], nrow = 1, byrow = T)
p7 <- as.matrix(pca_x[,7])
q7 <- matrix(pca_rotation[,7], nrow = 1, byrow = T)
p8 <- as.matrix(pca_x[,8])
q8 <- matrix(pca_rotation[,8], nrow = 1, byrow = T)
p9 <- as.matrix(pca_x[,9])
q9 <- matrix(pca_rotation[,9], nrow = 1, byrow = T)
p10 <- as.matrix(pca_x[,10])
q10 <- matrix(pca_rotation[,10], nrow = 1, byrow = T)


#### Prediction algorithm #9: Reg plus PC1 to PC10
new_pred_10 <- y3 + (p1%*%q1) + (p2%*%q2) + (p3%*%q3) + (p4%*%q4) + (p5%*%q5) + (p6%*%q6) + 
  (p7%*%q7) + (p8%*%q8) + (p9%*%q9) + (p10%*%q10)

rmse_PC1_to_10 <- sqrt(mean((val_ratings.m - new_pred_10)^2, na.rm = TRUE))

rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Reg plus PC1 to PC10", 
                                     RMSE = rmse_PC1_to_10))


# Summary table of all RMSE results for prediction algorithms used to date. 
rmse_results %>% knitr::kable()



### FINAL Prediction Algorithm
# As a final model I am taking that last algorithm (Reg plus PC1 to PC10) which is a 
# culmination of all preceding algorithms. A final assessment for the algorithm is to 
# test it against the validation testing dataset that has been kept aside during the 
# course of this work. 

# However as the last PCA section is based on matrix operations that must match dimensions 
# perfectly, some modifications are necessary to test it against the validation dataset. 

# First the validation dataset has to be put into matrix form to make it compatible with 
# the PCA work. 

## ----Validation to matrix--------------------------------------------------------------------------------
validation.m <- validation %>%
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()
rownames(validation.m) <- validation.m[,1]
validation.m <- validation.m[,-1]
colnames(validation.m) <- with(movie_titles, title[match(colnames(validation.m), movieId)])


## --------------------------------------------------------------------------------------------------------
unique_usr_val.f <- as.matrix(unique(validation$userId)) 
colnames(unique_usr_val.f) <- c("userId")


## --------------------------------------------------------------------------------------------------------
unique_mov_val.f <- as.matrix(unique(validation$movieId)) 
colnames(unique_mov_val.f) <- c("movieId")


## ----pca$x sparse matrix---------------------------------------------------------------------------------
pca_x.f <- pca$x %>% as.data.frame() %>%
  tibble::rownames_to_column(., "userId") %>%
  merge(unique_usr_val.f, ., by = "userId", all = TRUE)
pca_x.f <- as.matrix(pca_x.f[,-1])
pca_x.f[is.na(pca_x.f)] <- 0


## ----pca$rotation sparse matrix--------------------------------------------------------------------------
pca_rotation.f <- pca$rotation %>% as.data.frame() %>%
  tibble::rownames_to_column(., "movieId") %>%
  merge(unique_mov_val.f, ., by = "movieId", all = TRUE)
pca_rotation.f <- as.matrix(pca_rotation.f[,-1])
pca_rotation.f[is.na(pca_rotation.f)] <- 0


## ----algorithm 7 predictions using validation------------------------------------------------------------
predictions.f <- validation %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u)


## ----predictions.f to matrix-----------------------------------------------------------------------------
y3.f <- predictions.f %>%
  select(userId, movieId, pred) %>%
  spread(movieId, pred) %>%
  as.matrix()
rownames(y3.f) <- y3.f[,1]
y3.f <- y3.f[,-1]


## ----redefining p and q----------------------------------------------------------------------------------
p1 <- as.matrix(pca_x.f[,1])
q1 <- matrix(pca_rotation.f[,1], nrow = 1, byrow = T)
p2 <- as.matrix(pca_x.f[,2])
q2 <- matrix(pca_rotation.f[,2], nrow = 1, byrow = T)
p3 <- as.matrix(pca_x.f[,3])
q3 <- matrix(pca_rotation.f[,3], nrow = 1, byrow = T)
p4 <- as.matrix(pca_x.f[,4])
q4 <- matrix(pca_rotation.f[,4], nrow = 1, byrow = T)
p5 <- as.matrix(pca_x.f[,5])
q5 <- matrix(pca_rotation.f[,5], nrow = 1, byrow = T)
p6 <- as.matrix(pca_x.f[,6])
q6 <- matrix(pca_rotation.f[,6], nrow = 1, byrow = T)
p7 <- as.matrix(pca_x.f[,7])
q7 <- matrix(pca_rotation.f[,7], nrow = 1, byrow = T)
p8 <- as.matrix(pca_x.f[,8])
q8 <- matrix(pca_rotation.f[,8], nrow = 1, byrow = T)
p9 <- as.matrix(pca_x.f[,9])
q9 <- matrix(pca_rotation.f[,9], nrow = 1, byrow = T)
p10 <- as.matrix(pca_x.f[,10])
q10 <- matrix(pca_rotation.f[,10], nrow = 1, byrow = T)


## ----Final algorithm-------------------------------------------------------------------------------------
new_pred_10.f <- y3.f + (p1%*%q1) + (p2%*%q2) + (p3%*%q3) + (p4%*%q4) + (p5%*%q5) + (p6%*%q6) + 
  (p7%*%q7) + (p8%*%q8) + (p9%*%q9) + (p10%*%q10)

# Reporting RMSE value of final prediction algorithm 
rmse_final <- sqrt(mean((validation.m - new_pred_10.f)^2, na.rm = TRUE))
print("Final Algorithm RMSE")
rmse_final

